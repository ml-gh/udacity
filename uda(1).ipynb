{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_filenames = os.listdir('./train/train')\n",
    "train_cat = filter(lambda x:x[:3] == 'cat', train_filenames)\n",
    "train_dog = filter(lambda x:x[:3] == 'dog', train_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_filenames = os.listdir('./test/test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rmrf_mkdir(dirname):\n",
    "    if os.path.exists(dirname):\n",
    "        shutil.rmtree(dirname)\n",
    "    os.mkdir(dirname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# rmrf_mkdir('train2')\n",
    "# os.mkdir('train2/cat')\n",
    "# os.mkdir('train2/dog')\n",
    "\n",
    "# rmrf_mkdir('test2')\n",
    "# os.symlink('test/', 'test2/test')\n",
    "\n",
    "# for filename in train_cat:\n",
    "#     os.symlink('train/train/'+filename, 'train2/cat/'+filename)\n",
    "\n",
    "# for filename in train_dog:\n",
    "#     os.symlink('train/train/'+filename, 'train2/dog/'+filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# rmrf_mkdir('train2')\n",
    "# os.mkdir('train2/cat')\n",
    "# os.mkdir('train2/dog')\n",
    "\n",
    "# rmrf_mkdir('test2')\n",
    "\n",
    "# for filename in test_filenames:\n",
    "#     shutil.copy('./test/test/'+filename, './test2/')\n",
    "\n",
    "# for filename in train_cat:\n",
    "#     shutil.copy('./train/train/'+filename, './train2/cat/')\n",
    "\n",
    "# for filename in train_dog:\n",
    "#     shutil.copy('./train/train/'+filename, './train2/dog/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y= list(map(lambda x:1 if x[:3] == 'dog' else 0, train_filenames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(train_filenames, y, test_size=0.2, random_state=20277)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10006"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([filename for filename in X_train if filename[:3] == 'cat'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9994"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([filename for filename in X_train if filename[:3] == 'dog'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# rmrf_mkdir('train_img')\n",
    "# os.mkdir('train_img/cat')\n",
    "# os.mkdir('train_img/dog')\n",
    "\n",
    "# for filename in X_train:\n",
    "#     if filename[:3] == 'cat':\n",
    "#         shutil.copy('./train/train/'+filename, './train_img/cat/')\n",
    "#     else:\n",
    "#         shutil.copy('./train/train/'+filename, './train_img/dog/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# rmrf_mkdir('val_img')\n",
    "# os.mkdir('val_img/cat')\n",
    "# os.mkdir('val_img/dog')\n",
    "\n",
    "\n",
    "# for filename in X_val:\n",
    "#     if filename[:3] == 'cat':\n",
    "#         shutil.copy('./train/train/'+filename, './val_img/cat/')\n",
    "#     else:\n",
    "#         shutil.copy('./train/train/'+filename, './val_img/dog/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tune InceptionV3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.preprocessing import image\n",
    "from keras.models import Model\n",
    "from keras.layers import GlobalAveragePooling2D,Dense,Dropout\n",
    "from keras import backend as K\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.applications.inception_v3 import preprocess_input, decode_predictions\n",
    "from keras import optimizers\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "image_size=(299,299)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# gen = ImageDataGenerator(zoom_range=[0.8,1.2],\n",
    "#                          rotation_range=10,\n",
    "#                          width_shift_range=0.2,\n",
    "#                          height_shift_range=0.2,\n",
    "#                          preprocessing_function=preprocess_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gen = ImageDataGenerator(preprocessing_function=preprocess_input,       \n",
    "                            rotation_range=30,\n",
    "                            width_shift_range=0.2,\n",
    "                            height_shift_range=0.2,\n",
    "                            shear_range=0.2,\n",
    "                            zoom_range=0.2,\n",
    "                            horizontal_flip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bs=64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = gen.flow_from_directory(\"./train_img\", image_size, shuffle=False, batch_size=bs,class_mode= \"binary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "val_generator = gen.flow_from_directory(\"./val_img\", image_size, shuffle=False, batch_size=bs,class_mode= \"binary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create the base pre-trained model\n",
    "InceptionV3_base_model = InceptionV3(weights='imagenet', include_top=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "InceptionV3_x = InceptionV3_base_model.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "InceptionV3_x = GlobalAveragePooling2D()(InceptionV3_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "InceptionV3_x = Dense(1024, activation='relu')(InceptionV3_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "InceptionV3_x = Dropout(0.4)(InceptionV3_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "InceptionV3_predictions = Dense(1, activation='sigmoid')(InceptionV3_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "InceptionV3_model = Model(inputs = InceptionV3_base_model.input, outputs = InceptionV3_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for layer in InceptionV3_base_model.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "InceptionV3_model.compile(optimizer=optimizers.SGD(lr=0.005, momentum=0.9), loss='binary_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "313/312 [==============================] - 828s 3s/step - loss: 0.7135 - acc: 0.5630 - val_loss: 0.5898 - val_acc: 0.6904\n",
      "Epoch 2/5\n",
      "313/312 [==============================] - 581s 2s/step - loss: 0.6418 - acc: 0.6504 - val_loss: 0.5848 - val_acc: 0.5194\n",
      "Epoch 3/5\n",
      "313/312 [==============================] - 574s 2s/step - loss: 0.5859 - acc: 0.7107 - val_loss: 0.3840 - val_acc: 0.9302\n",
      "Epoch 4/5\n",
      "313/312 [==============================] - 580s 2s/step - loss: 0.4987 - acc: 0.7747 - val_loss: 0.3329 - val_acc: 0.8568\n",
      "Epoch 5/5\n",
      "313/312 [==============================] - 584s 2s/step - loss: 0.3742 - acc: 0.8523 - val_loss: 0.4998 - val_acc: 0.7440\n"
     ]
    }
   ],
   "source": [
    "InceptionV3_history_ = InceptionV3_model.fit_generator(train_generator,steps_per_epoch= len(X_train)/bs, epochs=5,verbose=1, validation_data=val_generator, validation_steps=len(X_val)/bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "InceptionV3_model.save('InceptionV3_base_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we chose to train the top 2 inception blocks, i.e. we will freeze\n",
    "# the first 249 layers and unfreeze the rest:\n",
    "\n",
    "for layer in InceptionV3_model.layers[:249]:\n",
    "    layer.trainable = False\n",
    "for layer in InceptionV3_model.layers[249:]:\n",
    "    layer.trainable = True   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_early_stop=EarlyStopping(monitor='val_loss', patience=5, verbose=1, mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "InceptionV3_model.compile(optimizer=optimizers.SGD(lr=0.0001, momentum=0.9), loss='binary_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "313/312 [==============================] - 590s 2s/step - loss: 0.2711 - acc: 0.9061 - val_loss: 0.3403 - val_acc: 0.8504\n",
      "Epoch 2/10\n",
      "313/312 [==============================] - 579s 2s/step - loss: 0.1746 - acc: 0.9718 - val_loss: 0.3496 - val_acc: 0.8402\n",
      "Epoch 3/10\n",
      "313/312 [==============================] - 582s 2s/step - loss: 0.1346 - acc: 0.9806 - val_loss: 0.3952 - val_acc: 0.8068\n",
      "Epoch 4/10\n",
      "313/312 [==============================] - 581s 2s/step - loss: 0.1091 - acc: 0.9866 - val_loss: 0.3529 - val_acc: 0.8252\n",
      "Epoch 5/10\n",
      "313/312 [==============================] - 578s 2s/step - loss: 0.0884 - acc: 0.9905 - val_loss: 0.3664 - val_acc: 0.8160\n",
      "Epoch 6/10\n",
      " 52/312 [===>..........................] - ETA: 5:52 - loss: 0.0755 - acc: 0.9931"
     ]
    }
   ],
   "source": [
    "InceptionV3_history = InceptionV3_model.fit_generator(train_generator, callbacks=[model_early_stop],steps_per_epoch= len(X_train)/bs, epochs=10,verbose=1, validation_data=val_generator, validation_steps=len(X_val)/bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "InceptionV3_model.save('InceptionV3_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "313/312 [==============================] - 371s 1s/step - loss: 0.1419 - acc: 0.9702 - val_loss: 0.3770 - val_acc: 0.8312\n",
      "Epoch 2/10\n",
      "313/312 [==============================] - 362s 1s/step - loss: 0.0994 - acc: 0.9873 - val_loss: 0.3683 - val_acc: 0.8358\n",
      "Epoch 3/10\n",
      "313/312 [==============================] - 362s 1s/step - loss: 0.0768 - acc: 0.9918 - val_loss: 0.3481 - val_acc: 0.8492\n",
      "Epoch 4/10\n",
      "313/312 [==============================] - 362s 1s/step - loss: 0.0616 - acc: 0.9941 - val_loss: 0.4285 - val_acc: 0.7752\n",
      "Epoch 5/10\n",
      "  8/312 [..............................] - ETA: 4:45 - loss: 0.0536 - acc: 1.0000"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-107-e06a51bf9351>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mInceptionV3_history\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInceptionV3_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_generator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel_early_stop\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mbs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_generator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mbs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   2242\u001b[0m                     outs = self.train_on_batch(x, y,\n\u001b[1;32m   2243\u001b[0m                                                \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2244\u001b[0;31m                                                class_weight=class_weight)\n\u001b[0m\u001b[1;32m   2245\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2246\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1888\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1889\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1890\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1891\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1892\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2473\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2474\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2475\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2476\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2477\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    903\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 905\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    906\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1135\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1136\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1137\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1138\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1353\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1354\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1355\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1356\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1357\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1359\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1360\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1361\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1362\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1363\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1338\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1339\u001b[0m           return tf_session.TF_Run(session, options, feed_dict, fetch_list,\n\u001b[0;32m-> 1340\u001b[0;31m                                    target_list, status, run_metadata)\n\u001b[0m\u001b[1;32m   1341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1342\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "InceptionV3_history = InceptionV3_model.fit_generator(train_generator, callbacks=[model_early_stop],steps_per_epoch= len(X_train)/bs, epochs=10,verbose=1, validation_data=val_generator, validation_steps=len(X_val)/bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "313/312 [==============================] - 331s 1s/step - loss: 0.7134 - acc: 0.5553 - val_loss: 0.6232 - val_acc: 0.5190\n",
      "Epoch 2/30\n",
      "313/312 [==============================] - 326s 1s/step - loss: 0.6303 - acc: 0.6538 - val_loss: 0.4946 - val_acc: 0.9276\n",
      "Epoch 3/30\n",
      "313/312 [==============================] - 321s 1s/step - loss: 0.5669 - acc: 0.7580 - val_loss: 0.4143 - val_acc: 0.8738\n",
      "Epoch 4/30\n",
      "313/312 [==============================] - 321s 1s/step - loss: 0.4892 - acc: 0.7916 - val_loss: 0.3670 - val_acc: 0.9182\n",
      "Epoch 5/30\n",
      "313/312 [==============================] - 321s 1s/step - loss: 0.3825 - acc: 0.8727 - val_loss: 0.4543 - val_acc: 0.7282\n",
      "Epoch 6/30\n",
      "313/312 [==============================] - 321s 1s/step - loss: 0.2588 - acc: 0.9256 - val_loss: 0.2040 - val_acc: 0.9380\n",
      "Epoch 7/30\n",
      "313/312 [==============================] - 321s 1s/step - loss: 0.1866 - acc: 0.9473 - val_loss: 0.2200 - val_acc: 0.9024\n",
      "Epoch 8/30\n",
      "313/312 [==============================] - 321s 1s/step - loss: 0.1375 - acc: 0.9671 - val_loss: 0.1757 - val_acc: 0.9248\n",
      "Epoch 9/30\n",
      "313/312 [==============================] - 321s 1s/step - loss: 0.1139 - acc: 0.9717 - val_loss: 0.1152 - val_acc: 0.9564\n",
      "Epoch 10/30\n",
      "313/312 [==============================] - 321s 1s/step - loss: 0.0726 - acc: 0.9845 - val_loss: 0.1779 - val_acc: 0.9228\n",
      "Epoch 12/30\n",
      "313/312 [==============================] - 321s 1s/step - loss: 0.0606 - acc: 0.9870 - val_loss: 0.2205 - val_acc: 0.9216\n",
      "Epoch 13/30\n",
      "313/312 [==============================] - 321s 1s/step - loss: 0.0515 - acc: 0.9906 - val_loss: 0.1124 - val_acc: 0.9592\n",
      "Epoch 14/30\n",
      "313/312 [==============================] - 320s 1s/step - loss: 0.0474 - acc: 0.9899 - val_loss: 0.1557 - val_acc: 0.9442\n",
      "Epoch 15/30\n",
      "313/312 [==============================] - 321s 1s/step - loss: 0.0385 - acc: 0.9935 - val_loss: 0.1415 - val_acc: 0.9450\n",
      "Epoch 16/30\n",
      "313/312 [==============================] - 321s 1s/step - loss: 0.0388 - acc: 0.9926 - val_loss: 0.1464 - val_acc: 0.9464\n",
      "Epoch 17/30\n",
      "313/312 [==============================] - 321s 1s/step - loss: 0.0311 - acc: 0.9954 - val_loss: 0.1421 - val_acc: 0.9474\n",
      "Epoch 18/30\n",
      "313/312 [==============================] - 321s 1s/step - loss: 0.0286 - acc: 0.9955 - val_loss: 0.1478 - val_acc: 0.9474\n",
      "Epoch 19/30\n",
      " 40/312 [==>...........................] - ETA: 3:40 - loss: 0.0265 - acc: 0.9961"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-f555035d3589>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mInceptionV3_history\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInceptionV3_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_generator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mbs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_generator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mbs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   2242\u001b[0m                     outs = self.train_on_batch(x, y,\n\u001b[1;32m   2243\u001b[0m                                                \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2244\u001b[0;31m                                                class_weight=class_weight)\n\u001b[0m\u001b[1;32m   2245\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2246\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1888\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1889\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1890\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1891\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1892\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2473\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2474\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2475\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2476\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2477\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    903\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 905\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    906\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1135\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1136\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1137\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1138\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1353\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1354\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1355\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1356\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1357\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1359\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1360\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1361\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1362\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1363\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1338\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1339\u001b[0m           return tf_session.TF_Run(session, options, feed_dict, fetch_list,\n\u001b[0;32m-> 1340\u001b[0;31m                                    target_list, status, run_metadata)\n\u001b[0m\u001b[1;32m   1341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1342\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "InceptionV3_history = InceptionV3_model.fit_generator(train_generator, steps_per_epoch= len(X_train)/bs, epochs=30,verbose=1, validation_data=val_generator, validation_steps=len(X_val)/bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_training(history): \n",
    "#     acc = history.history['acc'] \n",
    "#     val_acc = history.history['val_acc'] \n",
    "    loss = history.history['loss'] \n",
    "    val_loss = history.history['val_loss'] \n",
    "    epochs = range(len(loss)) \n",
    "#     plt.plot(epochs, acc, 'r.') \n",
    "#     plt.plot(epochs, val_acc, 'r') \n",
    "#     plt.title('Training and validation accuracy') \n",
    "#     plt.figure() \n",
    "    plt.plot(epochs, loss, 'b-') \n",
    "    plt.plot(epochs, val_loss, 'r-') \n",
    "    plt.title('Training and validation loss') \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_training(InceptionV3_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 input_6\n",
      "1 conv2d_471\n",
      "2 batch_normalization_471\n",
      "3 activation_471\n",
      "4 conv2d_472\n",
      "5 batch_normalization_472\n",
      "6 activation_472\n",
      "7 conv2d_473\n",
      "8 batch_normalization_473\n",
      "9 activation_473\n",
      "10 max_pooling2d_21\n",
      "11 conv2d_474\n",
      "12 batch_normalization_474\n",
      "13 activation_474\n",
      "14 conv2d_475\n",
      "15 batch_normalization_475\n",
      "16 activation_475\n",
      "17 max_pooling2d_22\n",
      "18 conv2d_479\n",
      "19 batch_normalization_479\n",
      "20 activation_479\n",
      "21 conv2d_477\n",
      "22 conv2d_480\n",
      "23 batch_normalization_477\n",
      "24 batch_normalization_480\n",
      "25 activation_477\n",
      "26 activation_480\n",
      "27 average_pooling2d_46\n",
      "28 conv2d_476\n",
      "29 conv2d_478\n",
      "30 conv2d_481\n",
      "31 conv2d_482\n",
      "32 batch_normalization_476\n",
      "33 batch_normalization_478\n",
      "34 batch_normalization_481\n",
      "35 batch_normalization_482\n",
      "36 activation_476\n",
      "37 activation_478\n",
      "38 activation_481\n",
      "39 activation_482\n",
      "40 mixed0\n",
      "41 conv2d_486\n",
      "42 batch_normalization_486\n",
      "43 activation_486\n",
      "44 conv2d_484\n",
      "45 conv2d_487\n",
      "46 batch_normalization_484\n",
      "47 batch_normalization_487\n",
      "48 activation_484\n",
      "49 activation_487\n",
      "50 average_pooling2d_47\n",
      "51 conv2d_483\n",
      "52 conv2d_485\n",
      "53 conv2d_488\n",
      "54 conv2d_489\n",
      "55 batch_normalization_483\n",
      "56 batch_normalization_485\n",
      "57 batch_normalization_488\n",
      "58 batch_normalization_489\n",
      "59 activation_483\n",
      "60 activation_485\n",
      "61 activation_488\n",
      "62 activation_489\n",
      "63 mixed1\n",
      "64 conv2d_493\n",
      "65 batch_normalization_493\n",
      "66 activation_493\n",
      "67 conv2d_491\n",
      "68 conv2d_494\n",
      "69 batch_normalization_491\n",
      "70 batch_normalization_494\n",
      "71 activation_491\n",
      "72 activation_494\n",
      "73 average_pooling2d_48\n",
      "74 conv2d_490\n",
      "75 conv2d_492\n",
      "76 conv2d_495\n",
      "77 conv2d_496\n",
      "78 batch_normalization_490\n",
      "79 batch_normalization_492\n",
      "80 batch_normalization_495\n",
      "81 batch_normalization_496\n",
      "82 activation_490\n",
      "83 activation_492\n",
      "84 activation_495\n",
      "85 activation_496\n",
      "86 mixed2\n",
      "87 conv2d_498\n",
      "88 batch_normalization_498\n",
      "89 activation_498\n",
      "90 conv2d_499\n",
      "91 batch_normalization_499\n",
      "92 activation_499\n",
      "93 conv2d_497\n",
      "94 conv2d_500\n",
      "95 batch_normalization_497\n",
      "96 batch_normalization_500\n",
      "97 activation_497\n",
      "98 activation_500\n",
      "99 max_pooling2d_23\n",
      "100 mixed3\n",
      "101 conv2d_505\n",
      "102 batch_normalization_505\n",
      "103 activation_505\n",
      "104 conv2d_506\n",
      "105 batch_normalization_506\n",
      "106 activation_506\n",
      "107 conv2d_502\n",
      "108 conv2d_507\n",
      "109 batch_normalization_502\n",
      "110 batch_normalization_507\n",
      "111 activation_502\n",
      "112 activation_507\n",
      "113 conv2d_503\n",
      "114 conv2d_508\n",
      "115 batch_normalization_503\n",
      "116 batch_normalization_508\n",
      "117 activation_503\n",
      "118 activation_508\n",
      "119 average_pooling2d_49\n",
      "120 conv2d_501\n",
      "121 conv2d_504\n",
      "122 conv2d_509\n",
      "123 conv2d_510\n",
      "124 batch_normalization_501\n",
      "125 batch_normalization_504\n",
      "126 batch_normalization_509\n",
      "127 batch_normalization_510\n",
      "128 activation_501\n",
      "129 activation_504\n",
      "130 activation_509\n",
      "131 activation_510\n",
      "132 mixed4\n",
      "133 conv2d_515\n",
      "134 batch_normalization_515\n",
      "135 activation_515\n",
      "136 conv2d_516\n",
      "137 batch_normalization_516\n",
      "138 activation_516\n",
      "139 conv2d_512\n",
      "140 conv2d_517\n",
      "141 batch_normalization_512\n",
      "142 batch_normalization_517\n",
      "143 activation_512\n",
      "144 activation_517\n",
      "145 conv2d_513\n",
      "146 conv2d_518\n",
      "147 batch_normalization_513\n",
      "148 batch_normalization_518\n",
      "149 activation_513\n",
      "150 activation_518\n",
      "151 average_pooling2d_50\n",
      "152 conv2d_511\n",
      "153 conv2d_514\n",
      "154 conv2d_519\n",
      "155 conv2d_520\n",
      "156 batch_normalization_511\n",
      "157 batch_normalization_514\n",
      "158 batch_normalization_519\n",
      "159 batch_normalization_520\n",
      "160 activation_511\n",
      "161 activation_514\n",
      "162 activation_519\n",
      "163 activation_520\n",
      "164 mixed5\n",
      "165 conv2d_525\n",
      "166 batch_normalization_525\n",
      "167 activation_525\n",
      "168 conv2d_526\n",
      "169 batch_normalization_526\n",
      "170 activation_526\n",
      "171 conv2d_522\n",
      "172 conv2d_527\n",
      "173 batch_normalization_522\n",
      "174 batch_normalization_527\n",
      "175 activation_522\n",
      "176 activation_527\n",
      "177 conv2d_523\n",
      "178 conv2d_528\n",
      "179 batch_normalization_523\n",
      "180 batch_normalization_528\n",
      "181 activation_523\n",
      "182 activation_528\n",
      "183 average_pooling2d_51\n",
      "184 conv2d_521\n",
      "185 conv2d_524\n",
      "186 conv2d_529\n",
      "187 conv2d_530\n",
      "188 batch_normalization_521\n",
      "189 batch_normalization_524\n",
      "190 batch_normalization_529\n",
      "191 batch_normalization_530\n",
      "192 activation_521\n",
      "193 activation_524\n",
      "194 activation_529\n",
      "195 activation_530\n",
      "196 mixed6\n",
      "197 conv2d_535\n",
      "198 batch_normalization_535\n",
      "199 activation_535\n",
      "200 conv2d_536\n",
      "201 batch_normalization_536\n",
      "202 activation_536\n",
      "203 conv2d_532\n",
      "204 conv2d_537\n",
      "205 batch_normalization_532\n",
      "206 batch_normalization_537\n",
      "207 activation_532\n",
      "208 activation_537\n",
      "209 conv2d_533\n",
      "210 conv2d_538\n",
      "211 batch_normalization_533\n",
      "212 batch_normalization_538\n",
      "213 activation_533\n",
      "214 activation_538\n",
      "215 average_pooling2d_52\n",
      "216 conv2d_531\n",
      "217 conv2d_534\n",
      "218 conv2d_539\n",
      "219 conv2d_540\n",
      "220 batch_normalization_531\n",
      "221 batch_normalization_534\n",
      "222 batch_normalization_539\n",
      "223 batch_normalization_540\n",
      "224 activation_531\n",
      "225 activation_534\n",
      "226 activation_539\n",
      "227 activation_540\n",
      "228 mixed7\n",
      "229 conv2d_543\n",
      "230 batch_normalization_543\n",
      "231 activation_543\n",
      "232 conv2d_544\n",
      "233 batch_normalization_544\n",
      "234 activation_544\n",
      "235 conv2d_541\n",
      "236 conv2d_545\n",
      "237 batch_normalization_541\n",
      "238 batch_normalization_545\n",
      "239 activation_541\n",
      "240 activation_545\n",
      "241 conv2d_542\n",
      "242 conv2d_546\n",
      "243 batch_normalization_542\n",
      "244 batch_normalization_546\n",
      "245 activation_542\n",
      "246 activation_546\n",
      "247 max_pooling2d_24\n",
      "248 mixed8\n",
      "249 conv2d_551\n",
      "250 batch_normalization_551\n",
      "251 activation_551\n",
      "252 conv2d_548\n",
      "253 conv2d_552\n",
      "254 batch_normalization_548\n",
      "255 batch_normalization_552\n",
      "256 activation_548\n",
      "257 activation_552\n",
      "258 conv2d_549\n",
      "259 conv2d_550\n",
      "260 conv2d_553\n",
      "261 conv2d_554\n",
      "262 average_pooling2d_53\n",
      "263 conv2d_547\n",
      "264 batch_normalization_549\n",
      "265 batch_normalization_550\n",
      "266 batch_normalization_553\n",
      "267 batch_normalization_554\n",
      "268 conv2d_555\n",
      "269 batch_normalization_547\n",
      "270 activation_549\n",
      "271 activation_550\n",
      "272 activation_553\n",
      "273 activation_554\n",
      "274 batch_normalization_555\n",
      "275 activation_547\n",
      "276 mixed9_0\n",
      "277 concatenate_11\n",
      "278 activation_555\n",
      "279 mixed9\n",
      "280 conv2d_560\n",
      "281 batch_normalization_560\n",
      "282 activation_560\n",
      "283 conv2d_557\n",
      "284 conv2d_561\n",
      "285 batch_normalization_557\n",
      "286 batch_normalization_561\n",
      "287 activation_557\n",
      "288 activation_561\n",
      "289 conv2d_558\n",
      "290 conv2d_559\n",
      "291 conv2d_562\n",
      "292 conv2d_563\n",
      "293 average_pooling2d_54\n",
      "294 conv2d_556\n",
      "295 batch_normalization_558\n",
      "296 batch_normalization_559\n",
      "297 batch_normalization_562\n",
      "298 batch_normalization_563\n",
      "299 conv2d_564\n",
      "300 batch_normalization_556\n",
      "301 activation_558\n",
      "302 activation_559\n",
      "303 activation_562\n",
      "304 activation_563\n",
      "305 batch_normalization_564\n",
      "306 activation_556\n",
      "307 mixed9_1\n",
      "308 concatenate_12\n",
      "309 activation_564\n",
      "310 mixed10\n"
     ]
    }
   ],
   "source": [
    "for i, layer in enumerate(InceptionV3_base_model.layers):\n",
    "    print(i, layer.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# at this point, the top layers are well trained and we can start fine-tuning\n",
    "# convolutional layers from inception V3. We will freeze the bottom N layers\n",
    "# and train the remaining top layers.\n",
    "\n",
    "# let's visualize layer names and layer indices to see how many layers\n",
    "# we should freeze:\n",
    "for i, layer in enumerate(base_model.layers):\n",
    "    print(i, layer.name)\n",
    "\n",
    "# we chose to train the top 2 inception blocks, i.e. we will freeze\n",
    "# the first 249 layers and unfreeze the rest:\n",
    "for layer in model.layers[:249]:\n",
    "    layer.trainable = False\n",
    "for layer in model.layers[249:]:\n",
    "    layer.trainable = True\n",
    "\n",
    "# we need to recompile the model for these modifications to take effect\n",
    "# we use SGD with a low learning rate\n",
    "from keras.optimizers import SGD\n",
    "model.compile(optimizer=SGD(lr=0.0001, momentum=0.9), loss='binary_crossentropy')\n",
    "\n",
    "# we train our model again (this time fine-tuning the top 2 inception blocks\n",
    "# alongside the top Dense layers\n",
    "model.fit_generator(...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 12500 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "test_generator = gen.flow_from_directory(\"./test\", image_size, shuffle=False, batch_size=32,class_mode= \"binary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 175s 449ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred = InceptionV3_model.predict_generator(test_generator, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.6747897 ],\n",
       "       [0.154649  ],\n",
       "       [0.1293558 ],\n",
       "       ...,\n",
       "       [0.23132463],\n",
       "       [0.14403033],\n",
       "       [0.1797391 ]], dtype=float32)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred = y_pred.clip(min=0.005, max=0.995)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.6747897 ],\n",
       "       [0.154649  ],\n",
       "       [0.1293558 ],\n",
       "       ...,\n",
       "       [0.23132463],\n",
       "       [0.14403033],\n",
       "       [0.1797391 ]], dtype=float32)"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# from keras.preprocessing.image import *\n",
    "\n",
    "df = pd.read_csv(\"./sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i, fname in enumerate(test_generator.filenames):\n",
    "    index = int(fname[fname.rfind('/')+1:fname.rfind('.')])\n",
    "    df.loc[index-1,'label'] = y_pred[i][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.to_csv('pred.csv', index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ypw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import *\n",
    "from keras.layers import *\n",
    "from keras.applications import *\n",
    "from keras.preprocessing.image import *\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "image_size=(299,299)\n",
    "width = image_size[0]\n",
    "height = image_size[1]\n",
    "input_tensor = Input((height, width, 3))\n",
    "x = input_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lambda_func' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-d510cdf1ff61>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0minput_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mif\u001b[0m \u001b[0mlambda_func\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLambda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlambda_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'lambda_func' is not defined"
     ]
    }
   ],
   "source": [
    "if lambda_func:\n",
    "    x = Lambda(lambda_func)(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.5/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "87916544/87910968 [==============================] - 1s 0us/step\n"
     ]
    }
   ],
   "source": [
    "base_model = InceptionV3(input_tensor=x, weights='imagenet', include_top=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.5/inception_v3_weights_tf_dim_ordering_tf_kernels.h5\n",
      "96116736/96112376 [==============================] - 1s 0us/step\n"
     ]
    }
   ],
   "source": [
    "model = InceptionV3(input_tensor = base_model.input, pooling= GlobalAveragePooling2D()(base_model.output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 25000 images belonging to 2 classes.\n",
      "Found 12500 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "gen = ImageDataGenerator()\n",
    "train_generator = gen.flow_from_directory(\"train2\", image_size, shuffle=False, \n",
    "                                          batch_size=16)\n",
    "test_generator = gen.flow_from_directory(\"test2\", image_size, shuffle=False, \n",
    "                                         batch_size=16, class_mode=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DirectoryIterator' object has no attribute 'nb_sample'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-c6bfbd307bb3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_generator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_generator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnb_sample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'DirectoryIterator' object has no attribute 'nb_sample'"
     ]
    }
   ],
   "source": [
    "train = model.predict_generator(train_generator, train_generator.nb_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = model.predict_generator(test_generator, test_generator.nb_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with h5py.File(\"gap_%s.h5\"%MODEL.func_name) as h:\n",
    "    h.create_dataset(\"train\", data=train)\n",
    "    h.create_dataset(\"test\", data=test)\n",
    "    h.create_dataset(\"label\", data=train_generator.classes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
